# Algorithms for Optimization

- Optimization Methods and Practices lecture in Kyonggi University (Fall Semester, 2020)
    - Textbook : Algorithms for Optimization (Mykel J.Kochenderfer and Tim A.Wheeler)
   
   
## What I felt in this lecture

- I felt the importance of hyper-parameter
    - In Jan.2020, I participated in the Kaggle competition "Boston house price prediction". I stayed all night by adjusting the hyper-parameter to lower RMSE.
    - At that time, I was only 2nd year in my major, and I didn't take a core course like regression, statistical theory, etc. So I didn't know RMSE.
    - After finishing the competition, I knew the "Grid-Search" method for finding the best hyper-parameter. And I thought It was a useless try to stay all night. The winner just implement one line code for what I did all night.
    - But, Taking an optimization class made me think this experience is not a useless try. Adjusting hyper-parameters directly and checking how to change modeling make me understand more optimization.


- I felt there are more theoretical things than I thought in one code line.
    - In the 2020-1st semester, I was in academic society and learned basic deep learning code.
    - I implemented optimization using just one line code with `tf.keras.optimizers.Adam` 
    - But In this lecture, I learned a developed process and many formulas for Adam. Simple code doesn't mean that the theory is simple. So, I thought I have to study both code and statistical theories.
    - It is important not only implementing code using packages, but also knowing what the code does.
        - Let's implement "code" based on "understanding"     
 

## Contents
1. Introduction
2. Derivate, Gradient
3. Bracketing
4. Bracketing
5. Local Descent
6. Local Descent
7. Code with sympy library
8. First-Order Methods
9. Gradient Descent
10. Gradient Descent
11. Second-Order Methods
12. Stochastic Methods
13. Genetic Algorithm
14. Surrogate Models

